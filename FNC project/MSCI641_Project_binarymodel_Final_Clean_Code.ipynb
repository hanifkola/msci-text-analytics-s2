{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSCI641-Project-binarymodel-Final Clean Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwtmFfCEINDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from keras.layers import GRU\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import array\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import load_model\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28L0IBcuIXxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5df43c23-c7bb-448c-a956-5fb760146ff1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rQXNRq_AP1Z",
        "colab_type": "text"
      },
      "source": [
        "Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64I1VOIeAVrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Max number of words in a sequence\n",
        "max_len_head = 20\n",
        "max_len_body = 80\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHBBuYhwNa_2",
        "colab_type": "text"
      },
      "source": [
        "Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWnW5mbBNph3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headers_train = pd.read_csv('/content/drive/My Drive/FNC-Project/train_stances.csv')\n",
        "bodies_train = pd.read_csv('/content/drive/My Drive/FNC-Project/train_bodies.csv')\n",
        "headers_test = pd.read_csv('/content/drive/My Drive/FNC-Project/competition_test_stances.csv')\n",
        "bodies_test = pd.read_csv('/content/drive/My Drive/FNC-Project/test_bodies.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqIKSPCfs8Bc",
        "colab_type": "text"
      },
      "source": [
        "***Filter Training Data*** Just for partially trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1DzCk13s4h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headers_train = headers_train[headers_train['Stance'] != 'unrelated']\n",
        "headers_train = headers_train.reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdt9jOZp7S_v",
        "colab_type": "text"
      },
      "source": [
        "**Cleaning and Tokenizing data and remove punctuation and Stop words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8dIzyk3L3Ob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "febd7abe-0f18-42c8-93fb-625cc2b39cb1"
      },
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "\n",
        "    return \" \".join(text)\n",
        "\n",
        "def counter_word(text):\n",
        "    count = Counter()\n",
        "    for i in text.values:\n",
        "        for word in i.split():\n",
        "            count[word] += 1\n",
        "    return count\n",
        "\n",
        "headers_test[\"Headline\"] = headers_test.Headline.map(lambda x: remove_punct(x))\n",
        "headers_train[\"Headline\"] = headers_train.Headline.map(lambda x: remove_punct(x))\n",
        "bodies_train[\"articleBody\"] = bodies_train.articleBody.map(lambda x: remove_punct(x))\n",
        "bodies_test[\"articleBody\"] = bodies_test.articleBody.map(lambda x: remove_punct(x))\n",
        "\n",
        "\n",
        "headers_test[\"Headline\"]= headers_test[\"Headline\"].map(remove_stopwords)\n",
        "headers_train[\"Headline\"]= headers_train[\"Headline\"].map(remove_stopwords)\n",
        "bodies_train[\"articleBody\"]= bodies_train[\"articleBody\"].map(remove_stopwords)\n",
        "bodies_test[\"articleBody\"]= bodies_test[\"articleBody\"].map(remove_stopwords)\n",
        "\n",
        "#Tokenizing training sets\n",
        "text = headers_train.Headline.append(bodies_train.articleBody) \n",
        "counter = counter_word(text)\n",
        "text = text.values.tolist()\n",
        "text_token = [line.split() for line in text]\n",
        "num_words = len(counter)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(headers_train.Headline.append(bodies_train.articleBody))\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_sequences_header = tokenizer.texts_to_sequences(headers_train.Headline)\n",
        "train_sequences_body = tokenizer.texts_to_sequences(bodies_train.articleBody)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxhMPvceccfe",
        "colab_type": "text"
      },
      "source": [
        "**Building Word2Vec model** with training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cINEaCtrcbX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Based on choice, one of this cell or the next one should be run\n",
        "w2vmodel = Word2Vec(\n",
        "        text_token,\n",
        "        size=50,\n",
        "        window=5,\n",
        "        min_count=1,\n",
        "        workers=4,\n",
        "    )\n",
        "W2vec_Status = 'with training set'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mS_CjlCAhB_c"
      },
      "source": [
        "**Building Word2Vec model** by importing Glove\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x3VrIlD0xOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "13019066-0e51-48ae-e82a-dffa1fb85839"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_file = datapath('/content/drive/My Drive/FNC-Project/glove.6B.50d.txt')\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
        "\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "w2vmodel = KeyedVectors.load_word2vec_format(tmp_file)\n",
        "\n",
        "W2vec_Status = 'Imported from the Glove.6B.50d' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rV_MjHUhQhD",
        "colab_type": "text"
      },
      "source": [
        "**Preparing Embedding matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mb-YSY6yAjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b6682772-5192-4018-e405-547b95e52572"
      },
      "source": [
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index) + 1, 50))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "        embeddings_vector = w2vmodel[word]\n",
        "    except KeyError:\n",
        "        embeddings_vector = None\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[i] = embeddings_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k69mr4vzrMHA",
        "colab_type": "text"
      },
      "source": [
        "**Creating input Data** Sequencing and padding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SVZBPdoeg8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70542d85-edb7-4da6-8b75-fc5f01d1b2f3"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_padded_head = pad_sequences(\n",
        "    train_sequences_header, maxlen=max_len_head, padding=\"post\", truncating=\"post\")\n",
        "train_padded_body = pad_sequences(\n",
        "    train_sequences_body, maxlen=max_len_body, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "train_padded = np.zeros((len(headers_train),max_len_head+max_len_body),dtype = 'i')\n",
        "for i in tqdm(range(0, len(headers_train),1)):\n",
        "  BodyID = headers_train[\"Body ID\"][i]\n",
        "  j = bodies_train[bodies_train[\"Body ID\"] == BodyID].index\n",
        "  train_padded[i] = np.append(train_padded_head[i] ,train_padded_body[j])\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 49972/49972 [00:40<00:00, 1222.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHUBsMC3jHeO",
        "colab_type": "text"
      },
      "source": [
        "**Preparing Labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EfuDPPIJOIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare labels based on the desired output of the model \n",
        "labels1 = pd.DataFrame()\n",
        "headers_train.loc[headers_train['Stance'] == 'unrelated', 'stance_id'] = 1\n",
        "headers_train.loc[headers_train['Stance'] == 'agree', 'stance_id'] = 2\n",
        "headers_train.loc[headers_train['Stance'] == 'disagree', 'stance_id'] = 3\n",
        "headers_train.loc[headers_train['Stance'] == 'discuss', 'stance_id'] = 4\n",
        "\n",
        "label_test = headers_train\n",
        "one_hot = pd.get_dummies(label_test['stance_id'])\n",
        "label_test = label_test.join(one_hot)\n",
        "x = label_test.filter(items = [1.0,2.0, 3.0, 4.0], axis = 1)\n",
        "labels = x.to_numpy()\n",
        "labels = labels.reshape(len(headers_train),4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hyWj65Nqd1a",
        "colab_type": "text"
      },
      "source": [
        "Building model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBeWhENGfcNE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "05a97f60-3e1e-4a98-98fc-e6dd08f4b33c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam , SGD\n",
        "\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(len(tokenizer.word_index)+1 ,50, weights = [embeddings_matrix], input_length=max_len_head + max_len_body))\n",
        "\n",
        "model1.add(LSTM(256, dropout=0.5))\n",
        "model1.add(Dense(128,activation = \"relu\", activity_regularizer=regularizers.l2(0.01)))\n",
        "model1.add(Dropout (0.2))\n",
        "model1.add(Dense(32,activation = \"relu\", activity_regularizer=regularizers.l2(0.01)))\n",
        "model1.add(Dropout (0.2))\n",
        "model1.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "#optimizer = SGD(lr = 1e-3, momentum = 0.9, decay = 0.01)\n",
        "optimizer = Adam(learning_rate=3e-4)\n",
        "model1.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 100, 50)           1486050   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 256)               314368    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 1,837,574\n",
            "Trainable params: 1,837,574\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njm-Cq_2NbBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "478c22e4-1564-41cb-969c-c601b3d33280"
      },
      "source": [
        "model = model1.fit(\n",
        "    train_padded, labels,batch_size = 64 ,epochs=10, validation_data=(train_padded, labels),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "781/781 [==============================] - 56s 72ms/step - loss: 0.3890 - accuracy: 0.7287 - val_loss: 0.3488 - val_accuracy: 0.7351\n",
            "Epoch 2/10\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 0.3415 - accuracy: 0.7394 - val_loss: 0.3148 - val_accuracy: 0.7573\n",
            "Epoch 3/10\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 0.3178 - accuracy: 0.7541 - val_loss: 0.2917 - val_accuracy: 0.7702\n",
            "Epoch 4/10\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 0.2985 - accuracy: 0.7682 - val_loss: 0.2740 - val_accuracy: 0.7890\n",
            "Epoch 5/10\n",
            "781/781 [==============================] - 54s 70ms/step - loss: 0.2827 - accuracy: 0.7858 - val_loss: 0.2596 - val_accuracy: 0.8059\n",
            "Epoch 6/10\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 0.2572 - accuracy: 0.8061 - val_loss: 0.2251 - val_accuracy: 0.8237\n",
            "Epoch 7/10\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 0.2265 - accuracy: 0.8229 - val_loss: 0.2045 - val_accuracy: 0.8398\n",
            "Epoch 8/10\n",
            "781/781 [==============================] - 53s 68ms/step - loss: 0.2047 - accuracy: 0.8399 - val_loss: 0.1868 - val_accuracy: 0.8527\n",
            "Epoch 9/10\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 0.1937 - accuracy: 0.8466 - val_loss: 0.1671 - val_accuracy: 0.8659\n",
            "Epoch 10/10\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 0.1805 - accuracy: 0.8583 - val_loss: 0.1601 - val_accuracy: 0.8699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62zrWn-vVGt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2298d08-9f95-4e74-8cd4-05fb442affb3"
      },
      "source": [
        "model1.save('/content/drive/My Drive/FNC-Project/model400-1.model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/FNC-Project/model400-1.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}